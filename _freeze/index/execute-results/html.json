{
  "hash": "98b619557f8f5d60e4900b015cfe0d0c",
  "result": {
    "markdown": "---\ntitle: \"The Problem with Hypothesis Testing\"\nsubtitle: \"Moving Away from Testing to Estimation\"\nauthor: \"Paul Johnson\"\n---\n\n\n# Hypothesis Testing {data-background-color=\"#425563\" data-verticator=\"#E8EDEE\"}\n\nThe Method I'm Telling You to Forget\n\n## The Logic of Scientific Reasoning {.center}\n\n- Hypotheses are claims about the world (the population) that we can empirically test using a sample of data.\n- We cannot prove our hypothesis about the population true based on what we find in our sample -- Deductive Reasoning.\n- We can only find enough evidence to reject a hypothesis (with a degree of certainty).\n- The Null Hypothesis Significance Testing approach gives us a null hypothesis (that nothing is going on) that we can treat as the default assumption about the world, that requires significant evidence to reject.\n\n## Hypothesis Testing Framework {.center}\n\n1. Specify the null hypothesis ($H_0$), alternative hypothesis ($H_1$), and significance level ($\\alpha$).\n2. Generate the null distribution, given the data being analysed and the type of test that is most appropriate for this data.\n3. Compute the test statistic that quantifies the difference between the null distribution and the observed data.\n4. Compute the p-value that represents the probability of observing a test statistic as large or larger than the observed test statistic if only chance generated the data.\n\n# Myriad Mistakes {data-background-color=\"#425563\" data-verticator=\"#E8EDEE\"}\n\nHow We Were Getting it Wrong All Along\n\n## The Replication Crisis {.center}\n\n- There are concerns about how much of published science can be replicated.\n- I think these concerns are overstated, but they do point to issues with trust, and there are real methodological concerns.\n\n## The Dreaded P-Value {.center}\n\n- The American Statistical Association (ASA) defines a p-value as \"the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value [@wasserstein2016].\n- Totally clear, right?\n\n## Statistical Power {.center}\n\n- We need to have confidence that tests are well specified, that the results are substantive and meaningful, and that the outcomes can be replicated.\n- We often forget about statistical power.\n- A lot of research lacks statistical power\n\n## Testing with Precision\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(htmltools)\n\nHTML('<section id = \"Probability of presence heatmap\" data-background-iframe=\"https://gallery.shinyapps.io/goog-trend-index/\" data-background-interactive></section>')\n```\n\n::: {.cell-output-display}\n```{=html}\n<section id = \"Probability of presence heatmap\" data-background-iframe=\"https://gallery.shinyapps.io/goog-trend-index/\" data-background-interactive></section>\n```\n:::\n:::\n\n\n## Why does \"Statistical Significance\" Matter?\n\n- Statistical significance is an indication of how surprised we are by what we observe in the data, given our assumption that the null hypothesis is true.\n- Does it really matter if we are surprised or not? \n- Yes! But it it's far from the only consideration, and it's not even the most important consideration.\n\n# A Path Through the Sewage {data-background-color=\"#425563\" data-verticator=\"#E8EDEE\"}\n\nDon't is **Still Not Enough**\n\n## Estimation vs Testing {.center}\n\n- Stop testing null hypotheses, start estimating meaningful quantities [@poole2022].\n- Testing null hypotheses doesn't tell us enough, and what it can tell us can't be relied upon.\n- Building statistical models that estimate effects can tell us what hypothesis testing claims to tell us, while being much more robust, and also telling us the effect sizes too.\n\n## Embracing Uncertainty {.center}\n\n- We have to accept that we are measuring effects with uncertainty, and we have to embrace the idea that our findings are uncertain.\n\n## Significant No More {.center}\n\n- We cannot and should not keep dichotomising p-values. A finding is not \"significant\" because p < 0.05, nor is a finding \"not significant\" because p > 0.05.\n\n# Thank You!\n\nContact:\n<br>\n\n<ul >\n{{< fa solid envelope >}} [paul.johnson50@nhs.net](mailto: paul.johnson50@nhs.net)\n</ul>\n\n\nSCW Data Science:\n<br>\n\n<ul >\n{{< fa brands github >}}[/NHS-South-Central-and-West](https://github.com/nhs-south-central-and-west/)\n</ul>\n\n## References\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    // dispatch for htmlwidgets\r\n    function fireSlideEnter() {\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n    }\r\n\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n      fireSlideEnter();\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}