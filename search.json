[
  {
    "objectID": "index.html#the-logic-of-scientific-reasoning",
    "href": "index.html#the-logic-of-scientific-reasoning",
    "title": "The Problem with Hypothesis Testing",
    "section": "The Logic of Scientific Reasoning",
    "text": "The Logic of Scientific Reasoning\n\nHypotheses are claims about the world (the population) that we can empirically test using a sample of data.\nWe cannot prove our hypothesis about the population true based on what we find in our sample – Deductive Reasoning.\nWe can only find enough evidence to reject a hypothesis (with a degree of certainty).\nThis is why hypothesis testing uses a null hypothesis and an alternative hypothesis."
  },
  {
    "objectID": "index.html#hypothesis-testing-framework",
    "href": "index.html#hypothesis-testing-framework",
    "title": "The Problem with Hypothesis Testing",
    "section": "Hypothesis Testing Framework",
    "text": "Hypothesis Testing Framework\n\nSpecify the null (\\(H_0\\)) and alternative (\\(H_1\\)) hypotheses, and significance level (\\(\\alpha\\)).\nGenerate the null distribution, given the data being analysed and the type of test that is most appropriate for this data.\nCompute the test statistic that describes the observed data, given the null distribution.\nCompute the p-value, the probability of observing a test statistic as large or larger than the observed test statistic if data generated by chance (and model assumptions are correct)."
  },
  {
    "objectID": "index.html#the-replication-crisis",
    "href": "index.html#the-replication-crisis",
    "title": "The Problem with Hypothesis Testing",
    "section": "The Replication Crisis",
    "text": "The Replication Crisis\n\nAttempts to replicate a lot of published scientific findings often fail.\nSome claims it is the majority of findings (Ioannidis 2005) and as many as 70% of researchers have tried and failed to reproduce experiments (Baker 2016).\nI think these concerns are overstated – but trust issues and methodological flaws are real."
  },
  {
    "objectID": "index.html#the-dreaded-p-value",
    "href": "index.html#the-dreaded-p-value",
    "title": "The Problem with Hypothesis Testing",
    "section": "The Dreaded P-Value",
    "text": "The Dreaded P-Value\n\nThe American Statistical Association (ASA) defines a p-value as “the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value (Wasserstein and Lazar 2016).\nWell, that’s cleared up any confusion… right?"
  },
  {
    "objectID": "index.html#testing-with-precision",
    "href": "index.html#testing-with-precision",
    "title": "The Problem with Hypothesis Testing",
    "section": "Testing with Precision",
    "text": "Testing with Precision\n\nWe need to have confidence that tests are well specified, that the results are substantive and meaningful, and that the outcomes can be replicated.\nScience has given too much weight to p-values.\nNot enough attention is paid to effect sizes, statistical power, and the garden of forking paths (Gelman and Loken 2021)."
  },
  {
    "objectID": "index.html#does-statistical-significance-matter",
    "href": "index.html#does-statistical-significance-matter",
    "title": "The Problem with Hypothesis Testing",
    "section": "Does “Statistical Significance” Matter?",
    "text": "Does “Statistical Significance” Matter?\n\nStatistical significance is an indication of how surprised we are by what we observe in the data, given our assumption that the null hypothesis is true.\nDoes it really matter if we are surprised or not?\nYes! But it is not the only consideration, and it’s not even the most important consideration."
  },
  {
    "objectID": "index.html#estimation-vs-testing",
    "href": "index.html#estimation-vs-testing",
    "title": "The Problem with Hypothesis Testing",
    "section": "Estimation vs Testing",
    "text": "Estimation vs Testing\n\nStop testing null hypotheses, start estimating meaningful quantities (Poole 2022).\nTesting null hypotheses doesn’t tell us enough, and what it can tell us might not be reliable.\nBuilding statistical models that estimate effects is more robust, more reliable, and more meaningful."
  },
  {
    "objectID": "index.html#embracing-uncertainty",
    "href": "index.html#embracing-uncertainty",
    "title": "The Problem with Hypothesis Testing",
    "section": "Embracing Uncertainty",
    "text": "Embracing Uncertainty\n\nWe have to accept that we are measuring effects with uncertainty, and we have to embrace the idea that our findings will include real-world variation.\nA “more refined goal of statistical analysis” is evaluating our uncertainty about the size of an effect (Greenland et al. 2016).\nReport effect sizes and their confidence intervals, use p-values for validation purposes."
  },
  {
    "objectID": "index.html#significant-no-more",
    "href": "index.html#significant-no-more",
    "title": "The Problem with Hypothesis Testing",
    "section": "Significant No More",
    "text": "Significant No More\n\nStop dichotomising p-values!\nA finding is not “significant” because p &lt; 0.05, nor is a finding “not significant” because p &gt; 0.05.\nStop referring to findings as “statistically significant” (or not statistically significant).\nReport the p-value, not an inequality."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "The Problem with Hypothesis Testing",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nBaker, Monya. 2016. “1,500 Scientists Lift the Lid on Reproducibility.” Nature 533 (7604).\n\n\nGelman, Andrew, and Eric Loken. 2021. “The Statistical Crisis in Science.” American Scientist 102 (6): 460. https://doi.org/10.1511/2014.111.460.\n\n\nGreenland, Sander, Stephen J. Senn, Kenneth J. Rothman, John B. Carlin, Charles Poole, Steven N. Goodman, and Douglas G. Altman. 2016. “Statistical Tests, p-Values, Confidence Intervals, and Power: A Guide to Misinterpretations.” European Journal of Epidemiology 31: 337–50.\n\n\nIoannidis, John. 2005. “Why Most Published Research Findings Are False.” PLoS Medicine 2 (8): e124.\n\n\nPoole, Charles. 2022. “The Statistical Arc of Epidemiology.” \"What is the Value of the P-Value?\" Panel Discussion; UNC TraCS, Duke University, and Wake Forest University CTSA BERD Cores.\n\n\nWasserstein, Ronald L., and Nicole A. Lazar. 2016. “The ASA Statement on p-Values: Context, Process, and Purpose.” The American Statistician 70 (2): 129–33. https://doi.org/10.1080/00031305.2016.1154108."
  }
]